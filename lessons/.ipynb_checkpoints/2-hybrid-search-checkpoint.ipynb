{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb8af4f7-2ea2-4187-ad1a-ef473e75214d",
   "metadata": {},
   "source": [
    "# Hybrid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009fa197-a4c3-4b9f-ac31-2a575c723170",
   "metadata": {},
   "source": [
    "### Dense vectors and Sparse Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644121ac-5d6d-4337-8ced-dd1fc17ecc69",
   "metadata": {},
   "source": [
    "* Dense Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465cc925-78fe-4892-8f14-74e67abe5993",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now so far with our embeddings we have been representing our text with **dense vectors**.  That is, when we submitted our text to OpenAI to be embedded, what was returned to us is a vector of 1536 entries, where each element represents a different characteristic encapsulating the *meaning* of the text. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a42e1ff-7d6c-46b9-8add-87452707f0bd",
   "metadata": {},
   "source": [
    "```python\n",
    "text_inputs = [\"There are good people here\"]\n",
    "\n",
    "MODEL = \"text-embedding-3-small\"\n",
    "\n",
    "res = client.embeddings.create(\n",
    "        input=text_inputs, model=MODEL\n",
    "    )\n",
    "    vectors = res.data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de112a7b-ab3d-4ded-bcc2-b6d29ff37739",
   "metadata": {},
   "source": [
    "This returns something like the following."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d51001e-39ba-4773-bc21-ba9091d664bd",
   "metadata": {},
   "source": [
    "```\n",
    "dense_vector = [-0.00281827, -0.00190404, -0.03839779,  0.08865113,  0.01493002,\n",
    "       -0.03636289, -0.07172307,  0.08416843, -0.01134681, -0.0448859 ,\n",
    "        0.01821093, -0.01549773,  0.02148448,  0.0063812 ,  0.04373574,\n",
    "        0.02649802, -0.01536502,  0.01486366,  0.00197961,  0.07797524,\n",
    "        0.03580255,  0.02223651,  0.0183289 ,  0.00479604,  0.02714683,\n",
    "        0.01404528, -0.02565751, -0.00899487,  0.02135177,  0.02487599,\n",
    "        0.01212096, -0.03798491,  0.00182294, -0.00522735, -0.01257808,\n",
    "       -0.01607281,  0.02760394, -0.01060216, -0.01015241, -0.03108393 ...]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fb547e-7990-4d36-bc8a-b218e79a03c0",
   "metadata": {},
   "source": [
    "We call the vector above a dense vector because every value in the vector is filled.  Notice that there are no zeros.  Instead, each entry represents a separate characteristic about the document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33bef0e-05b4-489b-bfc7-2d8a9bc0bb35",
   "metadata": {},
   "source": [
    "* Sparse Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590a194f-de72-4989-92d6-d0a0a02e8ada",
   "metadata": {},
   "source": [
    "However, there is a different way to represent a document.  A simpler way from a simpler time.  In this way, instead of each element representing element representing a different *characteristic* of a given text, it simply represents a different word.  If the word is present in the chunk of text, we represent that presence with a digit, and if the word is not in the chunk we represent that with a zero.  That's it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2968e07c-ae1e-4eb8-a325-2dde0c5521b3",
   "metadata": {},
   "source": [
    "For example, if you look at the `sparse_embedding.py` file you can see our code for reading in text from a .txt file on WWI into our vectorizer from sklearn.  If you look at one of the vectors from one of the vectors from one of the chunks, you'll see that it is almost all zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c4fa73-d55c-4cde-ab17-1458a6d70f33",
   "metadata": {},
   "source": [
    "```python\n",
    "tfidf_matrix.A[0][:50]\n",
    "\n",
    "# array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "#        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "#        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68873ac0-386b-4af9-bdb5-8a04dc940c27",
   "metadata": {},
   "source": [
    "Remember that this is because each element in the vector represents a different word, and only if a word is present in that chunk is the digit non-zero.\n",
    "\n",
    "> Don't worry, our vector doesn't have to represent all of the words in the English language, but instead will assign a different index to every word in our corpus (ie. all of the unique words that we feed into the model). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce965675-5e93-4e75-aca8-7cae49acdcc6",
   "metadata": {},
   "source": [
    "### Comparing Dense vs Sparse embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ed796c-4e84-490f-b928-8da399091988",
   "metadata": {},
   "source": [
    "Ok, so which one is better?  \n",
    "\n",
    "Well there are a lot of benefits to our original dense embeddings.  Dense embeddings are good at capturing nuanced semantic meanings, allowing for similarity searches that can identify relevant documents even if they don't share exact keywords -- ie semantic search. \n",
    "\n",
    "However, sparse embeddings come with a couple of benefits.  \n",
    "\n",
    "1. Sometimes, having an exact word for word match *is* important.\n",
    "2. It takes less time and computation to calculate a keyword match with our sparse embedding than to calculate a similarity score for every chunk in our corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d72c1c-fba7-4eef-a760-335134765364",
   "metadata": {},
   "source": [
    "### Introducing Hybrid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1488129e-1d8c-430e-8d4c-ba66dbad5011",
   "metadata": {},
   "source": [
    "So this is where hybrid search comes into play.  With hybrid search the entire corpus (ie. all of the chunks in our database) are represented with both dense and sparse embeddings.  Then when a query comes in, both a sparse and dense embedding is generated.  With the sparse embedding ideal keyword matching, and the dense embedding better for capturing the query's meaning.\n",
    "\n",
    "What does hybrid search look like with llamaindex?  Something like this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcbdbfa-712c-4542-bd80-5c5127b159dc",
   "metadata": {},
   "source": [
    "```python\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=2, sparse_top_k=10, vector_store_query_mode=\"hybrid\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d17ccf-606e-4d5a-9ce2-b358501d3ce2",
   "metadata": {},
   "source": [
    "So the, above favors keyword search over semantic search.  The top ten results are returned using the sparse embeddings, or keyword search, whereas only the top two are returned using the dense vectors or a semantic search. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10845a30-1923-4f42-b5dd-db4eb96a4b9a",
   "metadata": {},
   "source": [
    "### Seeing it in action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943b25f3-f15b-4b97-9d10-2c2186acbefb",
   "metadata": {},
   "source": [
    "Ok, so if you install the pip libraries, and then run `index.py` file, you can see the hybrid approach in action.\n",
    "\n",
    "```bash\n",
    "pip3 install -r requirements.text\n",
    "python3 -i index.py\n",
    "```\n",
    "\n",
    "We implemented it with Qdrant (pronounced quadrant) database.  It may take a while to run..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29039ae0-f916-4a9b-b74a-db83020da361",
   "metadata": {},
   "source": [
    "In the first few lines of code, we set up the qdrant database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7418cf-363c-4716-88f9-5ac2d945634e",
   "metadata": {},
   "source": [
    "```python\n",
    "client = QdrantClient(path=\"./qdrant_data\")\n",
    "vector_store = QdrantVectorStore(\n",
    "    \"ww1\", client=client, enable_hybrid=True, batch_size=20\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbdd2fa-4a08-45e9-b1d0-2a8381eb7621",
   "metadata": {},
   "source": [
    "> Notice that we have to specify `enable_hybrid=True` at the very beginning.  The batch size means that our chunks are embedded 20 at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbb826f-4927-484c-a278-848f1f68f7ce",
   "metadata": {},
   "source": [
    "And then we set up our query engine to use a hybrid search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6155513-f223-4aa5-ab58-d024aa9f50a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "```python\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=2, sparse_top_k=12, vector_store_query_mode=\"hybrid\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0268391c-430d-4df8-a229-939c7f88fa49",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efd86c9-0a04-42c0-b799-fbd5d6165f64",
   "metadata": {},
   "source": [
    "In this lesson, we learned about hybrid search.  With hybrid search, each chunk is embedded using both dense and sparse vectors.  Dense vectors are what we have been using so far, and with them, each element (ie dimension) represents a different characteristic of the text.  With sparse embeddings each dimension in a vector represents the presence or absence of a word.  Sparse embeddings are faster to query and favor exact word matches whereas dense embeddings favor semantic or matches based on meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582c63ee-f5f8-48c2-bd62-a250b19be515",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "[LLamaindex Qdrant Hybrid](https://docs.llamaindex.ai/en/stable/examples/vector_stores/qdrant_hybrid.html)\n",
    "\n",
    "[Pinecone Hybrid Search](https://www.pinecone.io/learn/hybrid-search-intro/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290d3542-6427-4a31-91e3-2942b36ee4fc",
   "metadata": {},
   "source": [
    "[Medium Text vs Vector](https://towardsdatascience.com/text-search-vs-vector-search-better-together-3bd48eb6132a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186e4166-1ab9-410e-bb9c-dac6d0dc3834",
   "metadata": {},
   "source": [
    "[Summary Advanced Techniques](https://medium.com/@sauravjoshi23/complex-query-resolution-through-llamaindex-utilizing-recursive-retrieval-document-agents-and-sub-d4861ecd54e6#:~:text=The%20Sub%20Question%20Query%20Engine%20aims%20at%20deconstructing%20a%20complex,information%20from%20relevant%20data%20sources.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
